# -*- coding: utf-8 -*-
"""Barge Cost Optimisation Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kY484kOE3vTCo4CwMi9UW8WqtuxBVdNk
"""

!pip install osmnx
!pip install ortools

!pip install pulp

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import folium
from folium import plugins
import plotly.express as px
import osmnx as ox
import networkx as nx
from ortools.constraint_solver import pywrapcp
from ortools.constraint_solver import routing_enums_pb2
from geopy.distance import geodesic

from google.colab import drive
drive.mount('/content/drive/')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive

Vessal_data = pd.read_csv('AIS_2016_01_Zone04 Filtered.csv')

Vessal_data.shape

Vessal_data.head(10)

# @title LAT vs LON

from matplotlib import pyplot as plt
Vessal_data.plot(kind='scatter', x='LAT', y='LON', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

# Get the unique vessel names from the first 100 rows
unique_vessels = Vessal_data.head(100)["VesselName"].unique()

# List to store DataFrames for each vessel
filtered_dataframes = []

# Process each vessel
for VesselName in unique_vessels:
    # Filter the data for the current vessel
    dtf = Vessal_data.head(100)[Vessal_data.head(100)["VesselName"] == VesselName][["VesselName", "MMSI", "LAT", "LON"]].reset_index(drop=True)

    # Reset the index and rename the columns
    dtf = dtf.reset_index().rename(columns={"index": "id", "LAT": "y", "LON": "x"})

    # Append the filtered DataFrame to the list
    filtered_dataframes.append(dtf)

    # Print the total number of rows for the current vessel
    print(f"Total number of rows for {VesselName}:", len(dtf))

    # Display the first three rows of the resulting DataFrame for the current vessel
    print(f"First 3 rows of the filtered data for {VesselName}:")
    print(dtf.head(3))

# If you need to concatenate all DataFrames into one, you can do:
all_vessels_data = pd.concat(filtered_dataframes, ignore_index=True)

all_vessels_data = pd.concat(filtered_dataframes, ignore_index=True)

# Display the number of unique vessel names in the result
num_unique_vessels = len(all_vessels_data["VesselName"].unique())
print(f"Number of unique vessel names in the result: {num_unique_vessels}")

def haversine_distance(lat1, lon1, lat2, lon2):
    """
    Calculate the great-circle distance between two points on the Earth's surfa
    ce.
    Parameters:
    lat1, lon1: Latitude and Longitude of point 1
    lat2, lon2: Latitude and Longitude of point 2
    Returns:
    Distance in kilometers
    """
    # Convert latitude and longitude from degrees to radians
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])

    # Haversine formula
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2
    c = 2 * np.arcsin(np.sqrt(a))

    # Radius of Earth in kilometers (mean radius)
    r = 6371.0
    return c * r

# Create a DataFrame to store distances
distance_matrix = pd.DataFrame(columns=["From", "To", "Distance_km"])

# Calculate distances between each pair of points
for i in range(len(all_vessels_data)):
    for j in range(i + 1, len(all_vessels_data)):
        lat1, lon1 = all_vessels_data.loc[i, ["y", "x"]]
        lat2, lon2 = all_vessels_data.loc[j, ["y", "x"]]
        distance = haversine_distance(lat1, lon1, lat2, lon2)
        distance_matrix = pd.concat([distance_matrix, pd.DataFrame({"From": [i], "To": [j], "Distance_km": [distance]})], ignore_index=True)

# Display the first few rows of the distance matrix
print("Distance Matrix (First 100 rows):")
print(distance_matrix.head(100))

# Define a threshold for significant difference in coordinates (in degrees)
threshold = 1  # Adjust this value as needed

# List to store significant differences
significant_differences = []

# Iterate through each pair of coordinates
for i in range(len(all_vessels_data)):
    for j in range(i + 1, len(all_vessels_data)):
        lat1, lon1 = all_vessels_data.loc[i, ["y", "x"]]
        lat2, lon2 = all_vessels_data.loc[j, ["y", "x"]]

        # Calculate the absolute difference in latitude and longitude
        lat_diff = abs(lat1 - lat2)
        lon_diff = abs(lon1 - lon2)

        # Check if either the latitude or longitude difference exceeds the threshold
        if lat_diff > threshold or lon_diff > threshold:
            significant_differences.append((i, j, lat1, lon1, lat2, lon2))

# Convert the list of significant differences to a DataFrame
significant_differences_df = pd.DataFrame(significant_differences, columns=["Index1", "Index2", "LAT1", "LON1", "LAT2", "LON2"])

# Display the significant differences
print("Significant Differences:")
print(significant_differences_df)

#Generate LAT and LON points for the data
import pandas as pd
import numpy as np

# Number of data points to generate
num_points = 100

# Generate random latitude and longitude data with significant differences
# Latitude ranges from -90 to 90 degrees
# Longitude ranges from -180 to 180 degrees
# Significant difference threshold is set to 10 degrees
threshold = 10

# Generate random LAT and LON values
LAT = np.random.uniform(low=-90, high=90, size=num_points)
LON = np.random.uniform(low=-180, high=180, size=num_points)

# Generate significant differences
significant_differences = []
for i in range(num_points):
    for j in range(i + 1, num_points):
        lat1, lon1 = LAT[i], LON[i]
        lat2, lon2 = LAT[j], LON[j]
        lat_diff = abs(lat1 - lat2)
        lon_diff = abs(lon1 - lon2)
        if lat_diff > threshold or lon_diff > threshold:
            significant_differences.append((i, j, lat1, lon1, lat2, lon2))

# Convert the list of significant differences to a DataFrame
significant_differences_df = pd.DataFrame(significant_differences, columns=["Index1", "Index2", "LAT1", "LON1", "LAT2", "LON2"])

# Display the significant differences
print("Significant Differences:")
print(significant_differences_df)

# DIstance matrix for all 5000
from geopy.distance import geodesic

# Calculate distances between pairs of points with significant differences
significant_distances = []

for _, row in significant_differences_df.iterrows():
    lat1, lon1 = row['LAT1'], row['LON1']
    lat2, lon2 = row['LAT2'], row['LON2']

    # Calculate distance using the Haversine formula
    distance = geodesic((lat1, lon1), (lat2, lon2)).kilometers
    significant_distances.append(distance)

# Convert the list of significant distances to a DataFrame
significant_distances_df = pd.DataFrame(significant_distances, columns=["Distance_km"])

# Display the significant distances
print("Significant Distances (in kilometers):")
print(significant_distances_df)

# DIstance matrix for 10
from geopy.distance import geodesic

# Calculate distances between pairs of points with significant differences
significant_distances = []

# Iterate through the first 10 rows of the DataFrame significant_differences_df
for _, row in significant_differences_df.head(10).iterrows():
    lat1, lon1 = row['LAT1'], row['LON1']
    lat2, lon2 = row['LAT2'], row['LON2']

    # Calculate distance using the Haversine formula
    distance = geodesic((lat1, lon1), (lat2, lon2)).kilometers
    significant_distances.append(distance)

# Convert the list of significant distances to a DataFrame
significant_distances_df = pd.DataFrame(significant_distances, columns=["Distance_km"])

# Display the significant distances
print("Significant Distances (in kilometers) for the first 10 pairs:")
print(significant_distances_df)

# Filter the dataset for the vessel "TIRA LANI"
tira_lani_data = all_vessels_data[all_vessels_data["VesselName"] == "TIRA LANI"]

# Print the rows corresponding to the vessel "TIRA LANI"
print(tira_lani_data)

# Filter out duplicate entries for vessel names and only consider one entry per vessel name
unique_vessel_data = all_vessels_data.drop_duplicates(subset=["VesselName"])

# Print the first few rows of the unique vessel data
print("Unique Vessel Data:")
print(unique_vessel_data.head())

# Now you can proceed with further processing using the unique vessel data

from geopy.distance import geodesic
import numpy as np
import pandas as pd
# Function to calculate Haversine distance between two points
def haversine_distance(lat1, lon1, lat2, lon2):
    return geodesic((lat1, lon1), (lat2, lon2)).kilometers
# Example vessel names and locations
vessel_names = unique_vessel_data["VesselName"].tolist()[:15]  # Taking the first 15 vessel names from the unique data
locations = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']
# Example distances (you should replace these with actual distances)
# Assuming distances are calculated using haversine_distance function
example_distances = np.random.rand(len(vessel_names), len(locations))
# Create a dictionary to store the cost matrix
Distance_matrix = {}
# Fill in the cost matrix
for i, vessel in enumerate(vessel_names):
    Distance_matrix[vessel] = {}
    for j, location in enumerate(locations):
        # Assume vessel latitude and longitude are taken from the initial dataset
        vessel_lat = unique_vessel_data.loc[unique_vessel_data["VesselName"] == vessel, "y"].iloc[0]
        vessel_lon = unique_vessel_data.loc[unique_vessel_data["VesselName"] == vessel, "x"].iloc[0]
        # Assume location latitude and longitude are taken from the generated data
        location_lat = LAT[j]
        location_lon = LON[j]
        # Calculate the distance using Haversine formula
        distance = haversine_distance(vessel_lat, vessel_lon, location_lat, location_lon)
        # Assign the distance to the cost matrix
        Distance_matrix[vessel][location] = distance
# Convert the cost matrix to a DataFrame
df_Distance_matrix = pd.DataFrame(Distance_matrix)
# Display the DataFrame
print("Distance_matrix:")
print(df_Distance_matrix)

import random

# Number of locations
num_locations = 10

# Generate demand for each location (whole numbers)
demands = [random.randint(1000, 5000) for _ in range(num_locations)]

# Assuming a constant cost per tons per kilometer
cost_per_tons_per_km = 100  # Example cost, you can adjust this value as needed

# Create a new DataFrame to store the total costs
df_total_costs = df_Distance_matrix * cost_per_tons_per_km * np.array(demands)[:, np.newaxis]

# Display the table with demand and total cost for each vessel
print("Vessel\tLocation\tDemand (tons)\tTotal Cost ($)")
print("-" * 55)

for vessel in df_total_costs.columns:
    for i, location in enumerate(df_total_costs.index):
        total_cost = df_total_costs.loc[location, vessel]
        print(f"{vessel}\t{location}\t{demands[i]}\t\t{total_cost:.2f}")
# Export the DataFrame to an Excel file
excel_file = "transportation_costs.xlsx"
df_total_costs.to_excel(excel_file, index=False)

print(f"Data has been saved to {excel_file}")

import pandas as pd
import numpy as np
from pulp import LpProblem, LpVariable, lpSum, LpMinimize
import random

# Number of locations
num_locations = 10

# Generate demand for each location (whole numbers)
demands = [random.randint(1000, 5000) for _ in range(num_locations)]

# Assuming a constant cost per tons per kilometer
cost_per_tons_per_km = 100  # Example cost, can adjust this value as needed

# Assuming df_Distance_matrix is the distance matrix DataFrame from the first code
# Create a new DataFrame to store the total costs
df_total_costs = df_Distance_matrix * cost_per_tons_per_km * np.array(demands)[:, np.newaxis]

# Display the table with demand and total cost for each vessel
print("Vessel\tLocation\tDemand (tons)\tTotal Cost ($)")
print("-" * 55)

# Export the DataFrame to an Excel file
excel_file = "transportation_costs.xlsx"
df_total_costs.to_excel(excel_file, index=False)
print(f"Data has been saved to {excel_file}")

# Updated vessel names and market regions
vessels = [
    'TIRA LANI', '95SC1203', 'MARY CATHERINE', 'PERSISTENCE LAB', 'GUTSY LADY 4',
    'NIOLO', 'CG GALVESTON ISLAND', 'CAPT LES EASOM', 'NAVAJO', 'CASSIOPEIA',
    'LEO SUN', 'STARR', 'ILIWAI', 'HOKU-KE\'A', 'MIKIOI'
]

market_region = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']

# Demand in 1000 tons
demand = {region: demand / 1000 for region, demand in zip(market_region, demands)}

# Supply is initially set to 0 as it's not provided in the original data
supply = {vessel: 0 for vessel in vessels}

# Capacity values in 1000 tons
capacity = {
    'TIRA LANI': 5, '95SC1203': 5, 'MARY CATHERINE': 5, 'PERSISTENCE LAB': 5, 'GUTSY LADY 4': 5,
    'NIOLO': 5, 'CG GALVESTON ISLAND': 6, 'CAPT LES EASOM': 6, 'NAVAJO': 6, 'CASSIOPEIA': 6,
    'LEO SUN': 7, 'STARR': 7, 'ILIWAI': 7, 'HOKU-KE\'A': 7, 'MIKIOI': 7
}

# Fixed costs are inferred as average of the total costs for each vessel
fixed_cost = {vessel: sum(costs) / len(costs) for vessel, costs in df_total_costs.iterrows()}

# Create transportation cost dictionary from the provided data
transportation_cost = {}
for region_idx, region in enumerate(market_region):
    transportation_cost[region] = {vessel: df_total_costs.loc[region, vessel] for vessel in vessels}

# Normalize vessel names
fixed_cost_normalized = {vessel.upper(): cost for vessel, cost in fixed_cost.items()}
transportation_cost_normalized = {region: {vessel.upper(): cost for vessel, cost in costs.items()} for region, costs in transportation_cost.items()}

def solve1(vessels, market_region):
    # Create the PuLP optimization problem
    model = LpProblem("Supply_Chain", LpMinimize)

    # Create decision variables
    quantity = LpVariable.dicts("quantity", (vessels, market_region), lowBound=0, cat='Continuous')

    # Add constraints - demand
    for r in market_region:
        model += lpSum(quantity[s][r] for s in vessels) == demand[r], f'demand_{r}'

    # Add constraints - capacity
    for s in vessels:
        model += lpSum(quantity[s][r] for r in market_region) <= capacity[s], f'capacity_{s}'

    # Define the objective function
    obj = lpSum(fixed_cost_normalized[s] for s in fixed_cost_normalized) + \
          lpSum(transportation_cost_normalized[r][s] * quantity[s][r] for s in vessels for r in market_region)

    # Set the objective
    model += obj

    # Solve the optimization problem
    model.solve()

    # Print the results
    if model.status == 1:  # 1 indicates an optimal solution was found
        total_cost = sum(fixed_cost_normalized[s] for s in fixed_cost_normalized) + \
                     sum(transportation_cost_normalized[r][s] * quantity[s][r].value() for s in vessels for r in market_region)
        print(f"Total Cost: ${total_cost:,.2f}")
        for s in vessels:
            for r in market_region:
                if quantity[s][r].value() > 0:
                    print(f"{s} ships to {r}: {quantity[s][r].value()} units")
    else:
        print("No solution found.")

# Call the optimization function
solve1(vessels, market_region)

import random

# Number of locations
num_locations = 10

# Generate demand for each location (whole numbers)
demands = [random.randint(1000, 5000) for _ in range(num_locations)]

# Assuming a constant cost per tons per kilometer
cost_per_tons_per_km = 100  # Example cost, you can adjust this value as needed

# Create a new DataFrame to store the total costs
df_total_costs = df_Distance_matrix * cost_per_tons_per_km * np.array(demands)[:, np.newaxis]

# Display the table with demand and total cost for each vessel
print("Vessel\tLocation\tDemand (tons)")
print("-" * 55)

for vessel in df_total_costs.columns:
    for i, location in enumerate(df_total_costs.index):
        total_cost = df_total_costs.loc[location, vessel]
        print(f"{vessel}\t{location}\t{demands[i]}")
# Export the DataFrame to an Excel file
excel_file = "transportation_costs.xlsx"
df_total_costs.to_excel(excel_file, index=False)

print(f"Data has been saved to {excel_file}")